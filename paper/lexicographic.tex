\section{Lexicographic Supermartingales}
\label{sec:lexicographic}

In this section we introduce the notion of a \emph{lexicographic ranking 
supermartingale}, which generalizes the standard notion of ranking 
supermartingales. However, to define any form of a supermartingale, we need the crucial notion of conditional expectation.

\smallskip\noindent{\bf Conditional Expectation.} 
%The notion of conditional expectation 
%tightly connected to martingales. We present only a brief informal exposition 
%here, for formal treatment see, e.g.~\cite[Chapter 9]{Williams:book}.
Let $(\Omega,\mathcal{F},\probm)$ be a probability space, 
$X\colon\Omega\rightarrow 
\Rset$ an $\mathcal{F}$-measurable function, and $\mathcal{F}'\subseteq 
\mathcal{F}$ sub-sigma-algebra of $\mathcal{F}$. A \emph{conditional 
	expectation} of $X$ given $\mathcal{F}'$ is an $\mathcal{F}'$-measurable random 
variable denoted by $\E[X| \mathcal{F}']$ which satisfies, for each set $A\in 
\mathcal{F}'$, the following: 
\begin{equation}
\label{eq:cond-exp}
\E[X\cdot 1_A] = \E[\E[X|\mathcal{F}]\cdot 1_A],
\end{equation}
where $1_A \colon \Omega\rightarrow \{0,1\}$ is an \emph{indicator function} of 
$A$, i.e. function returning $1$ for 
each $\omega\in A$ and $0$ for each $\omega\in \Omega\setminus A$. Note that 
the left hand-side of~\eqref{eq:cond-exp} intuitively represents the expected 
value 
of $X(\omega)$ with domain restricted to $A$.

Note that any $\mathcal{F}'$-measurable random variable satisfying~\eqref{eq:cond-exp} can be called a conditional expectation. The definition does not guarantee that the conditional expectation is uniquely defined or that it exists at all. However, from probability theory we have the following:

\begin{proposition}
\label{prop:conditional-exp-existence}
Let $(\Omega,\mathcal{F},\probm)$ be a probability space, 
$X\colon\Omega\rightarrow 
\Rset$ an $\mathcal{F}$-measurable function, and $\mathcal{F}'\subseteq 
\mathcal{F}$ sub-sigma-algebra of $\mathcal{F}$. Assume that one of the following conditions hold:
\begin{itemize}
\item $\E[|X|]<\infty$; or
\item $X$ is real-valued and non-negative.
\end{itemize}
Then there exists a conditional expectation of $X$ given $\mathcal{F}'$ and it is almost-surely unique, i.e. for each two $\mathcal{F}'$-measurable functions $f$, $g$ that satisfy~\eqref{eq:cond-exp} it holds $\probm(\{\omega\mid f(\omega)\neq g(\omega)\})=0$.
\end{proposition}
\begin{proof}
The proof for the case when $\E[|X|]$ is standard and appears in many textbooks on probability theory (e.g.~\cite{Billingsley:book,Ash:book,Rosenthal:book}). The proof for the second case is essentially the same: the condition that $X$ is non-negative and not admitting infinite value suffices for satisfying the assumptions of Radon-Nikodym Theorem, the main theoretical tool used in the proof. For the sake of completeness we present the proof in \AppendixMaterial.
\end{proof}

Since the constraint~\eqref{eq:cond-exp} defining conditional expectation is phrased in terms of expected values, the almost-sure uniqueness cannot be strengthened to uniqueness, as re-defining a random variable on a set of probability zero does not change its expectation. In the following, when we say that a conditional expectation of a random variable $X$ satisfies some inequality (e.g. $\E[X\mid\mathcal{F}]\geq 0$) on set $L\subseteq \Omega$, we mean that for each $\mathcal{F}$-measurable function $\E[X\mid\mathcal{F}]$ satisfying~\eqref{eq:cond-exp} the .

In context of probabilistic programs we work with probability 
spaces of the form $(\Omega,\natfilt,\probm^\sigma)$, where $\Omega$ is a 
set of runs in some $\pCFG$ and $\natfilt$ is (the smallest) sigma-algebra 
such that all the functions $\cfg{\sigma}{i}$, where $i\in \Nset_0$ and 
$\sigma$ is a scheduler, are $\natfilt$-measurable. In such a setting we can 
also consider sub-sigma-algebras $\natfilt_i$, $i\in \Nset_0$, of 
$\natfilt$, where $\natfilt_i$ is the smallest sub-sigma-algebra of 
$\natfilt$ such that all the functions $\cfg{\sigma}{j}$, $0\leq j \leq 
i$, are $\natfilt_i$-measurable. Intuitively, each set $A$ belonging to such 
an $\natfilt_i$ consists of runs whose first $i$ steps satisfy some 
property, and the probability space $(\Omega,\natfilt_i,\probm^\sigma)$ 
allows us to reason about probabilities of certain events happening in the 
first 
$i$ steps of program execution. 
%(note that since $\natfilt_i \subseteq 
%\natfilt$, we can view $\probm^\sigma$ also as a function of type 
%$\natfilt_i\rightarrow [0,1]$). 
Then, for each $A\in \natfilt_i$, the 
value $\E[\E[X|\natfilt_i]\cdot 1_A]$ represents the expected value of 
$X(\run)$ for the randomly generated run $\run$ provided that we restrict to 
runs whose
prefix of length $i$ satisfies the property given by $A$. 
The sequence $\natfilt_0,\natfilt_1,\natfilt_2,\dots$ forms a 
filtration of $\natfilt$, which we call a \emph{canonical filtration}.

\begin{definition}
Let $(\Omega,\genfilt,\probm)$ be a probability space and $O\subseteq \Omega$ some set.
 We say that sets $L_1,\dots,L_n$ form an almost-sure partition of $O$ if $L_1,\dots,L_n$ are pairwise disjoint and $\probm(\{\omega\in O\mid \omega \not \in L_1\cup\dots \cup L_n\})=0$
\end{definition}

\begin{definition}[Lexicographic Ranking Supermartingale]
\label{def:lexrsm}
Let $(\Omega,\genfilt,\probm)$ be a probability space, 
$\{\genfilt_i\}_{i=0}^{\infty}$ a filtration of $\genfilt$, $\stime$ a stopping 
time w.r.t. that filtration, and 
$\eps\geq 0$. 
An $n$-dimensional real-valued stochastic process 
$\{\vec{X}_{i}\}_{i=0}^{\infty}$ is a 
\emph{lexicographic $\eps$-ranking supermartingale for $\stime$ 
($\eps$-LexRSM)} if the 
following 
conditions hold:
\begin{compactenum}
\item For each $1\leq j \leq n$ the 1-dimensional stochastic process 
$\{\vecseq{X}{i}{j}\}_{i=0}^{\infty}$ is adapted to 
$\{\genfilt_i\}_{i=0}^{\infty}$.
%\item For each $i\in \Nset_0$ and each $1\leq j \leq n$ it holds 
%$\E[|\vecseq{X}{i}{j}|]<\infty$.
\item For each $\omega \in \Omega$, $i\in \Nset_0$ and $1\leq j \leq n$ it holds 
$\vec{X}_i (\omega)[j]\geq {0}$. % and $\vec{X}_i(\omega)[j]\neq \infty$.
\item For each $i\in \Nset_0$ there exists an almost-sure partition of the set $\{\stime>i\}$ into $n$ subsets $L^i_1,\dots,L^i_n$, all of them $\genfilt_i$-measurable, such that for each $1\leq j \leq n$:
\begin{compactitem}
	\item $\E[\vecseq{X}{i+1}{j}\mid 
	\genfilt_i]\leq\vecseq{X}{i}{j} - 
	\eps$ on $L_j$; and
	\item for all $1 \leq j' < j$ we have $\E[\vecseq{X}{i+1}{j'}\mid 
	\genfilt_i]=\vecseq{X}{i}{j'}$ on $L_{j}$.
\end{compactitem}
\end{compactenum}
\end{definition}

The 1-dimensional lexicographic $\eps$-ranking supermartingale is, to a large extent, equivalent to the notion of a ranking supermartingale as studied in~\cite{xxx}. There is one significant difference: in~\cite{xxx} there is an additional \emph{integrability} condition imposed on the one-dimensional process $\{X_i\}_{i=0}^{\infty}$, which requires that for each $i\geq 0$ it holds $\E[|X_i|]<\infty$ (or equivalently $\E[X_i]<\infty$, as the process is required to be non-negative). We do not impose this condition, and indeed, it is possible to define a 3-dimensional LexRSM $\{\vec{X}_{i}\}_{i=0}^{\infty}$ such that $\E[X[3]_i]=\infty$ for some $i$ \textbf{[PETR: EXAMPLE]}. However, closer inspection of proofs in~\cite{xxx} that link 1-dimensional ranking supermartingales to a.s. termination reveals that the integrability condition is never explicitly used there. Indeed, the only point in which integrability is needed is to ensure that the conditional expectations exist and are well-defined. However, the existence of conditional expectations is also guaranteed for random variables that are non-negative, even if their expected value if infinite, see Theorem~\ref{xxx}. This is exactly the case in both the original 1-dimensional ranking supermartingales and our generalization to LexRSMs. Below we show, that also in the LexRSM case we do not need integrability to prove a.s. termination, and hence we drop the condition altogether. This is particularly relevant when applying LexRSMs to programs with non-linear arithmetic, where proving integrability of LexRSM is more difficult than in the affine case, see Example~\ref{xxx}. 

\begin{theorem}
Let $(\Omega,\genfilt,\probm)$ be a probability space, 
$\{\genfilt_i\}_{i=0}^{\infty}$ a filtration of $\genfilt$, and $\stime$ a 
stopping 
time w.r.t. that filtration. If there exists an $\eps$-LexRSM for $\stime$, 
then $\probm(\stime<\infty)=1$.
\end{theorem}
\begin{proof}
The proof proceeds by contradiction, i.e. we assume that an $\eps$-LexRSM for 
$\stime$ exists and $\probm(\stime=\infty)>0$. For succinctness we denote the 
set $\{\omega\mid \stime(\omega)=\infty\}$ by $\genRunSet_{\infty}$.

For $\omega\in \Omega$ we define the level of $\omega$ at step $i$ to be the 
number $\levelrank{\omega}{i}$ equal to either $0$, if $\stime(\omega)\leq i$ or $\omega$ does not belong to $L^i_1 \cup \dots\cup L^i_n$, 
or otherwise to the $1\leq j \leq n$ such that $\omega\in L^i_j$. By the definition of 
$\eps$-LexRSM the value $\levelrank{\omega}{i}$ is well-defined for all 
$\omega$ and moreover, the random variable $\levelrank{}{i}$ is $\genfilt_i$-measurable. We denote by $\minlev(\omega)$ the smallest $0 \leq j \leq n$ such 
that $j$ is a level of $\omega$ at infinitely many steps. Note that $\omega \in 
\genRunSet_{\infty}$ implies $\minlev(\omega)\neq 0$, and $\probm(\genRunSet_{\infty})=\probm(\{\minlev \neq 0 \})$, as for each $i$ almost all $\omega$'s for which $\stime(\omega)>i$ belong to $L^i_1 \cup \dots\cup L^i_n$. We denote by $M_i$ 
the set of all $\omega$'s with $\minlev(\omega)=i$.

Throughout the proof we use several times the following fundamental fact: if 
$\probm(A)>0$ for some set $A$ and $A=A_1\cup A_2 \cup A_3\cdots$ for some 
sequence of sets $A_1,A_2,A_3,\dots$, then there exists $i$ such that 
$\probm(A_i)>0$.

Now since $\genRunSet_{\infty}=M_1,\dots,M_n$, there must be $1\leq \fixn{j} \leq n$ 
s.t. $\probm(M_{\fixn{j}})>0$. For each $\omega\in M_{\fixn{j}}$ there is the smallest number
${i}_{\omega,\fixn{j}}\in\Nset_0$ such that for all $i\geq {i}_{\omega,{j}}$ it holds 
$\levelrank{\omega}{i}\geq \fixn{j}$. Denote by $S_{\fixn{j},{i}}$ the set of all $\omega$'s in $M_{\fixn{j}}$ s.t. 
$i_{\omega,\fixn{j}}={i}$. 
Since $M_{\fixn{j}} = M_{\fixn{j},1} \cup M_{\fixn{j},2} \cup M_{\fixn{j},3} \cup \cdots$, there is $\fixn{i}\in 
\Nset_0$ 
s.t. 
$\probm(M_{\fixn{j},\fixn{i}})>0$. Continuing on the same note, for each $B\in \Nset$ we 
denote by $M_{\fixn{j},\fixn{i}}^{B}$ the set off all $\omega$'s in $M_{\fixn{j},\fixn{i}}$ s.t. 
$\vecseq{X}{\fixn{i}}{\fixn{j}}(\omega)\leq B$. Since $M_{\fixn{j},\fixn{i}}= M_{\fixn{j},\fixn{i}}^{1} \cup M_{\fixn{j},\fixn{i}}^{2} 
\cup M_{\fixn{j},\fixn{i}}^{3} \cup \cdots  $, there is $\fixn{B}\in \Nset$ s.t. 
$\probm(M_{\fixn{j},\fixn{i}}^{\fixn{B}})>0$. Denote $\fixn{p}=\probm(M_{\fixn{j},\fixn{i}}^{\fixn{B}})$.

Let $D$ be the set of all $\omega\in \Omega$ such that 
$\vecseq{X}{\fixn{i}}{\fixn{j}}(\omega)\leq \fixn{B}$. Note that $M_{\fixn{j},\fixn{i}}^{B} \subseteq D$ and $D\in 
\genfilt_i$ (and thus also $D\in \genfilt_{i'}$ for all $i'\geq i$). 
Define a stopping time $U$ w.r.t. filtration $\{\genfilt_i\}_{i=0}^{\infty}$ as follows: for all $\omega \in \Omega$ 
we put 
$U(\omega)=\inf\{k\in \Nset_0 \mid k\geq \fixn{i} \text{ and }
\levelrank{\omega}{k} < \fixn{j} \}$.

Define a 
(one-dimensional) stochastic process $\{Y_k\}_{k=0}^{\infty}$ as 
follows:
$$
Y_k(\omega) = \begin{dcases}
0 & \text{if $\omega \not\in D$}\\
\fixn{B} & \text{if $\omega \in D$ and $k<\fixn{i}$ }\\
\vecseq{X}{k}{\fixn{j}}(\omega) & \text{if $\omega \in D$, $k\geq \fixn{i}$ and 
$U(\omega)>k$ 
}\\
\vecseq{X}{U(\omega)}{\fixn{j}}(\omega) & \text{if $\omega\in D$, $k\geq 
\fixn{i}$ and 
$U(\omega)\leq k$ }.
\end{dcases}
$$

We prove several properties of the process $\{Y_k\}_{k=0}^{\infty}$. 
First, clearly for all $k\geq 0$, $Y_k(\omega)\geq 0$. Second, for each $k\geq \fixn{i}$, the variable $Y_k$ is $\genfilt_k$-measurable, as $D\in \genfilt_{\fixn{i}}$, $\{\vecseq{X}{i}{\fixn{j}}(\omega)\}_{i=0}^{\infty}$ is adapted to the filtration $\{\genfilt_i\}_{i=0}^{\infty}$ and $U$ is a stopping time w.r.t. this filtration. Finally, for any $k\in \Nset_0$ denote by 
$\noofdec_k$ the random variable such that $\noofdec_k(\omega) = |\{i'\in 
\Nset\mid \fixn{i} \leq i' < k \text{ and } \levelrank{\omega}{i'}=\fixn{j}\} |$.
We claim 
that for each $k\geq \fixn{i}$ 
it holds 
\begin{equation}
\label{eq:lexrsm-soundness-main}
\E[Y_k]\leq \fixn{B}\cdot \probm(D) - \eps\cdot\sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D 
\cap \{U\geq k\} \cap \{\noofdec_k 
= 
\ell\}).
\end{equation}

The proof of~\eqref{eq:lexrsm-soundness-main} goes by induction on $k$. For 
$k=\fixn{i}$ the sum on the right-hand side equals $0$, so the 
inequality immediately follows from the definition of $Y_k$. Now assume 
that~\eqref{eq:lexrsm-soundness-main} holds for some $k\geq \fixn{i}$. We have that 
\begin{align}
\label{eq:lexrsm-ind-1}
\E[Y_{k+1}] &= \underbrace{\E[Y_{k+1}\cdot \indicator{\Omega\setminus D}]}_{=0=\E[Y_k\cdot\indicator{\Omega\setminus D}]} +  \underbrace{\E[Y_{k+1}\cdot \indicator{D \cap \{U \leq k\}}]}_{=\E[Y_k\cdot\indicator{D\cap \{U\leq k\}}]} + \underbrace{\E[Y_{k+1}\cdot \indicator{D \cap \{U > k\}}]}_{=\E[\vecseq{X}{k+1}{\fixn{j}}\cdot\indicator{D\cap \{U>k \} }]},
\end{align}
where the equality $\E[Y_{k+1}\cdot \indicator{D \cap \{U \leq k\}}] =\E[Y_k\cdot\indicator{D\cap \{U\leq k\}}] $ follows from the fact that $Y_{k+1}(\omega)=Y_k({\omega})=\vecseq{X}{U(\omega)}{\fixn{j}}(\omega)$ for $\omega\in \{U\leq k\}$, and similarly for the last term. We prove that 
\begin{equation}
\label{eq:lexrsm-ind-2}
\E[\vecseq{X}{k+1}{\fixn{j}}\cdot \indicator{D\cap \{U > k \}}] \leq \E[Y_k\cdot \indicator{D\cap \{U > k \}} -\eps\cdot \indicator{D\cap \{U>k\} \cap \{\levelrank{}{k}= \fixn{j}\} }] .
\end{equation}
Indeed, it holds
\begin{equation}
\label{eq:lexrsm-ind-3}
\E[\vecseq{X}{k+1}{\fixn{j}}\cdot \indicator{D\cap \{U > k \}}] = \E[\vecseq{X}{k+1}{\fixn{j}}\cdot \indicator{D\cap \{U > k \} \cap \{\levelrank{}{k}=\fixn{j} \} }]  + \E[\vecseq{X}{k+1}{\fixn{j}}\cdot \indicator{D\cap \{U > k \} \cap \{\levelrank{}{k}>\fixn{j}\} }], 
\end{equation}
since $\levelrank{\omega}{k}\geq \fixn{j}$ for all $\omega \in \{U>k\}$. Since the set $D\cap \{U > k \} \cap \{\levelrank{}{k}=\fixn{j} \}$ is $\genfilt_k$-measurable, we get
\begin{align}
\E[\vecseq{X}{k+1}{\fixn{j}}\cdot \indicator{D\cap \{U > k \} \cap \{\levelrank{}{k}=\fixn{j} \} }] &= \E[\E[\vecseq{X}{k+1}{\fixn{j}}\mid \genfilt_k]\cdot \indicator{D\cap \{U > k \} \cap \{\levelrank{}{k}=\fixn{j} \} }] \label{eq:lexrsm-ind-4}\\
&\leq \E[(\vecseq{X}{k}{\fixn{j}} - \eps)\cdot \indicator{D\cap \{U > k \} \cap \{\levelrank{}{k}=\fixn{j} \}}] \label{eq:lexrsm-ind-5}\\
&=\E[(Y_k - \eps)\cdot \indicator{D\cap \{U > k \} \cap \{\levelrank{}{k}=\fixn{j} \}}] \label{eq:lexrsm-ind-6},
\end{align}
where~\eqref{eq:lexrsm-ind-4} follows from the definition of conditional expectation~\eqref{eq:cond-exp},~\eqref{eq:lexrsm-ind-5} follows from the definition of $\{\levelrank{}{k}=\fixn{j}\}$, and~\eqref{eq:lexrsm-ind-6} holds since $Y_k(\omega)=\vecseq{X}{k}{\fixn{j}}(\omega)$ for $\omega$ with $U(\omega)> k$. Almost identical argument shows that
\begin{equation}
\label{eq:lexrsm-ind-7}
\E[\vecseq{X}{k+1}{\fixn{j}}\cdot \indicator{D\cap \{U > k \} \cap \{\levelrank{}{k}>\fixn{j} \} }] \leq \E[Y_k\cdot \indicator{D\cap \{U > k \} \cap \{\levelrank{}{k}>\fixn{j} \}}].
\end{equation}
Plugging~\eqref{eq:lexrsm-ind-6} and~\eqref{eq:lexrsm-ind-7} into~\eqref{eq:lexrsm-ind-3} yields~\eqref{eq:lexrsm-ind-2}. Now we can plug~\eqref{eq:lexrsm-ind-2} into~\eqref{eq:lexrsm-ind-1} to get
\begin{align}
\E[Y_{k+1}]&\leq \E[Y_k] - \eps\cdot \E[\indicator{D \cap \{U>k\} \cap \{\levelrank{}{k} = \fixn{j} \} }] = \E[Y_k] - \eps\cdot \probm(D \cap \{U>k\} \cap \{\levelrank{}{k} = \fixn{j} \} ) \nonumber\\
&\leq \fixn{B}\cdot \probm(D) - \eps\cdot\left(\sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D 
\cap \{U\geq k\} \cap \{\noofdec_k = \ell\})\right) -\eps\cdot \probm(D \cap \{U>k\} \cap \{\levelrank{}{k} = \fixn{j} \}),
\end{align}
where the last inequality follows from induction hypothesis. Hence, using $D_{k,\ell}$ as a shorthand for $D 
\cap \{U\geq k\} \cap \{\noofdec_k = \ell\}$, to prove~\eqref{eq:lexrsm-soundness-main} it remains to show that
\begin{equation}
\label{eq:lexrsm-ind-8}
\sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D_{k,\ell}) + \probm(D \cap \{U>k\} \cap \{\levelrank{}{k} = \fixn{j} \}) = \sum_{\ell=0}^{k+1-\fixn{i}} \ell\cdot\probm(D_{k+1,\ell}).
\end{equation}
The left-hand side of~\eqref{eq:lexrsm-ind-8} is equal to
\begin{align}
&\phantom{+}\;\sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D_{k,\ell} \cap \{\levelrank{}{k}=\fixn{j}\}) + \sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D_{k,\ell}\cap \{\levelrank{}{k}>\fixn{j}\} )\nonumber \\ 
&+\sum_{\ell=0}^{k-\fixn{i}}\probm(\underbrace{D \cap \{U>k\} \cap \{\levelrank{}{k} = \fixn{j}\} \cap \{\noofdec_k = \ell\}}_{=D_{k,\ell} \cap \{\levelrank{}{k} = \fixn{j}\}})
\nonumber \\
&= \sum_{\ell=0}^{k-\fixn{i}}(\ell+1) \cdot\probm(D_{k,\ell}\cap \{\levelrank{}{k} = \fixn{j}\}) +  \sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D_{k,\ell}\cap \{\levelrank{}{k}>\fixn{j}\} ) \nonumber\\
&=\sum_{\ell=0}^{k-\fixn{i}} (\ell+1)\cdot \probm{(D_{k+1,\ell+1} \cap \{\levelrank{}{k} = \fixn{j}\})} +  \sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D_{k+1,\ell}\cap \{\levelrank{}{k}>\fixn{j}\} ) \label{eq:lexrsm-ind-9}\\
&= \sum_{\ell=1}^{k+1-\fixn{i}} \ell\cdot \probm{(D_{k+1,\ell} \cap \{\levelrank{}{k} = \fixn{j}\})} +  \sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D_{k+1,\ell}\cap \{\levelrank{}{k}>\fixn{j}\} )
   \nonumber \\
&= (k+1-\fixn{i})\cdot \probm{(D_{k+1,k+1-\fixn{i}}\cap\{\levelrank{}{k} =\fixn{j} \} )} + \sum_{\ell=1}^{k-\fixn{i}} \ell \cdot \probm(D_{k+1,\ell})  \nonumber\\
&= (k+1-\fixn{i})\cdot\probm(D_{k+1,k+1-\fixn{i}}) + \sum_{\ell=1}^{k-\fixn{i}}\ell\cdot\probm(D_{k+1,\ell}) = \text{ right-hand side of~\eqref{eq:lexrsm-ind-8}} \label{eq:lexrsm-ind-11}.
\end{align}
The individual steps in the above computation are justified as follows: in~\eqref{eq:lexrsm-ind-9} we use the facts that for all $\omega$'s whose level in step $k$ is $\fixn{j}$ it holds that $\noofdec_k(\omega)+1=\noofdec_{k+1}(\omega)$, and similarly, for $\omega$'s whose level in step $k$ is $>\fixn{j}$ it holds $\noofdec_k(\omega)=\noofdec_{k+1}(\omega)$. Moreover, for all $\omega\in D_{k,\ell}$ it holds that $\levelrank{k}{\omega}\geq \fixn{j} \Rightarrow U(\omega)\geq k+1$. Finally, in~\eqref{eq:lexrsm-ind-11} we use the fact that all $\omega\in D_{k+1,k+1-\fixn{i}}$ need to have level $\fixn{j}$ in step $k$, since otherwise such an $\omega$ would need to have level $\fixn{j}$ for at least $k+1-\fixn{i}$ times within steps $\{\fixn{i},\fixn{i}+1,\dots,k-1\}$, but there are $k-\fixn{i}$ such steps, a contradiction. This concludes the proof of~\eqref{eq:lexrsm-soundness-main}.

Now according to~\eqref{eq:lexrsm-soundness-main} it holds  $\E[Y_k]\leq \fixn{B}\cdot \probm(D) - \eps\cdot\sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D 
\cap \{U\geq k\} \cap \{\noofdec_k 
= 
\ell\})$ for all $k\geq \fixn{i}$. Let $m=3\fixn{B}\cdot \probm(D)/(\eps\cdot \fixn{p})$. For each $\omega\in M_{\fixn{j},\fixn{i}}^{\fixn{B}} $ there exists $k_m(\omega)\geq \fixn{i}$ such that $\noofdec_{k_{m}(\omega)}\geq m$. Hence, $M_{\fixn{j},\fixn{i}}^{\fixn{B}} = \bigcup_{k=\fixn{i}}^{\infty} M_{\fixn{j},\fixn{i}}^{\fixn{B}} \cap \{k_{m}(\omega) \leq k\}$ and $\fixn{p}=\probm(M_{\fixn{j},\fixn{i}}^{\fixn{B}})=\lim_{k\rightarrow \infty}\probm(M_{\fixn{j},\fixn{i}}^{\fixn{B}} \cap \{k_{m} \leq k\})$. Thus, there exists $k_0$ such that $\probm(M_{\fixn{j},\fixn{i}}^{\fixn{B}} \cap \{\noofdec_{k_0}\geq m\}) \geq \fixn{p}/2$. Denote $\fixn{M}= M_{\fixn{j},\fixn{i}}^{\fixn{B}} \cap \{\noofdec_{k_0}\geq m\}$. Clearly $\fixn{M} \subseteq D\cap \{U\geq k_0 \} \cap \{\noofdec_{k_0}\geq m \}$. From \eqref{eq:lexrsm-soundness-main} it follows that
\begin{align*}
\E[Y_{k_0}]&\leq \fixn{B}\cdot \probm(D) - \eps\cdot\sum_{\ell=0}^{k_0-\fixn{i}} \ell\cdot\probm(D 
\cap \{U\geq k_0\} \cap \{\noofdec_{k_0} 
=
\ell\}) \\
&= \fixn{B}\cdot \probm(D) - \eps\cdot \sum_{\ell=1}^{k_0-\fixn{i}} \probm(D 
\cap \{U\geq k_0\} \cap \{\noofdec_{k_0} 
\geq 
\ell\})\\
& \leq \fixn{B}\cdot \probm(D) - \eps\cdot \sum_{\ell=1}^{m} \probm(D 
\cap \{U\geq k_0\} \cap \{\noofdec_{k_0} 
\geq 
\ell\})\\
&\leq \fixn{B}\cdot \probm(D) - \eps\cdot m \cdot \fixn{p}/2 < 0, 
\end{align*}
the last inequality following by substituting for $m$. But for each $k$ the random variable $Y_k$, so it must also have a non-negative expectation, a contradiction.
%\begin{enumerate}
%\
%\end{itemize}
%\end{enumerate}


%\begin{fact}
	
%\end{fact}


%\[
%\levelrank{\omega}{i} = \begin{dcases}
%0 & \text{if $\stime(\omega)\leq i$} \\
% 
%\end{dcases}
%\]
\end{proof}

