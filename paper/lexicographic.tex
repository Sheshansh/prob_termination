\section{Lexicographic Supermartingales}
\label{sec:lexicographic}

In this section we introduce the notion of a \emph{lexicographic ranking 
supermartingale}, which generalizes the standard notion of ranking 
supermartingales. However, to define any form of a supermartingale, we need the crucial notion of conditional expectation.

\smallskip\noindent{\bf Conditional Expectation.} 
%The notion of conditional expectation 
%tightly connected to martingales. We present only a brief informal exposition 
%here, for formal treatment see, e.g.~\cite[Chapter 9]{Williams:book}.
Let $(\Omega,\mathcal{F},\probm)$ be a probability space, 
$X\colon\Omega\rightarrow 
\Rset$ an $\mathcal{F}$-measurable function, and $\mathcal{F}'\subseteq 
\mathcal{F}$ sub-sigma-algebra of $\mathcal{F}$. A \emph{conditional 
	expectation} of $X$ given $\mathcal{F}'$ is an $\mathcal{F}'$-measurable random 
variable denoted by $\E[X| \mathcal{F}']$ which satisfies, for each set $A\in 
\mathcal{F}'$, the following: 
\begin{equation}
\label{eq:cond-exp}
\E[X\cdot 1_A] = \E[\E[X|\mathcal{F}]\cdot 1_A],
\end{equation}
where $1_A \colon \Omega\rightarrow \{0,1\}$ is an \emph{indicator function} of 
$A$, i.e. function returning $1$ for 
each $\omega\in A$ and $0$ for each $\omega\in \Omega\setminus A$. Note that 
the left hand-side of~\eqref{eq:cond-exp} intuitively represents the expected 
value 
of $X(\omega)$ with domain restricted to $A$.

Note that any $\mathcal{F}'$-measurable random variable satisfying~\eqref{eq:cond-exp} can be called a conditional expectation. The definition does not guarantee that the conditional expectation is uniquely defined or that it exists at all. However, from probability theory we have the following:

\begin{proposition}
\label{prop:conditional-exp-existence}
\label{PROP:CONDITIONAL-EXP-EXISTENCE}
Let $(\Omega,\mathcal{F},\probm)$ be a probability space, 
$X\colon\Omega\rightarrow 
\Rset$ an $\mathcal{F}$-measurable function, and $\mathcal{F}'\subseteq 
\mathcal{F}$ sub-sigma-algebra of $\mathcal{F}$. Assume that one of the following conditions hold:
\begin{itemize}
\item $\E[|X|]<\infty$; or
\item $X$ is real-valued and non-negative.
\end{itemize}
Then there exists a conditional expectation of $X$ given $\mathcal{F}'$ and it is almost-surely unique, i.e. for each two $\mathcal{F}'$-measurable functions $f$, $g$ that satisfy~\eqref{eq:cond-exp} it holds $\probm(\{\omega\mid f(\omega)\neq g(\omega)\})=0$.
\end{proposition}
%\textbf{[PETR: POLISH, ADD INTUITION]}
\begin{proof}[Proof (Key ideas)]
The proof for the case when $\E[|X|]$ is standard and appears in many textbooks on probability theory (e.g.~\cite{Billingsley:book,Ash:book,Rosenthal:book}). The proof for the second case is essentially the same: the condition that $X$ is non-negative and not admitting infinite value suffices for satisfying the assumptions of Radon-Nikodym Theorem, the main theoretical tool used in the proof. For the sake of completeness we present the proof in \AppendixMaterial.
\end{proof}

Since the constraint~\eqref{eq:cond-exp} defining conditional expectation is 
phrased in terms of expected values, the almost-sure uniqueness cannot be 
strengthened to uniqueness, as re-defining a random variable on a set of 
probability zero does not change its expectation. In the following, when we say 
that a conditional expectation of a random variable $X$ satisfies some 
inequality (e.g. $\E[X\mid\mathcal{F}]\geq 0$) on set $L\subseteq \Omega$, we 
mean that for each $\mathcal{F}$-measurable function $\E[X\mid\mathcal{F}]$ 
satisfying~\eqref{eq:cond-exp} the inequality holds on some subset $L'\subseteq 
L$ such that $\probm(L')=\probm(L)$.

\begin{remark}
\label{rem:canfilt}
In context of probabilistic programs we work with probability 
spaces of the form $(\Omega,\natfilt,\probm^\sigma)$, where $\Omega$ is a 
set of runs in some $\pCFG$ and $\natfilt$ is (the smallest) sigma-algebra 
such that all the functions $\cfg{\sigma}{i}$, where $i\in \Nset_0$ and 
$\sigma$ is a scheduler, are $\natfilt$-measurable. In such a setting we can 
also consider sub-sigma-algebras $\natfilt_i$, $i\in \Nset_0$, of 
$\natfilt$, where $\natfilt_i$ is the smallest sub-sigma-algebra of 
$\natfilt$ such that all the functions $\cfg{\sigma}{j}$, $0\leq j \leq 
i$, are $\natfilt_i$-measurable. 
Intuitively, each set $A\in\natfilt_i$ consists of runs whose first $i$ steps 
satisfy some property.
%property, and the probability space $(\Omega,\natfilt_i,\probm^\sigma)$ 
%allows us to reason about probabilities of certain events happening in the 
%first 
%$i$ steps of program execution. 
%(note that since $\natfilt_i \subseteq 
%\natfilt$, we can view $\probm^\sigma$ also as a function of type 
%$\natfilt_i\rightarrow [0,1]$). 
Then, for each $A\in \natfilt_i$, the 
value $\E[\E[X|\natfilt_i]\cdot 1_A]$ represents the expected value of 
$X(\run)$ for the randomly generated run $\run$ provided that we restrict to 
runs whose
prefix of length $i$ satisfies the property given by $A$. 
The sequence $\natfilt_0,\natfilt_1,\natfilt_2,\dots$ forms a 
filtration of $\natfilt$, which we call a \emph{canonical filtration}.
\end{remark}

%\begin{definition}
%Let $(\Omega,\genfilt,\probm)$ be a probability space and $O\subseteq \Omega$ some set.
% We say that sets $L_1,\dots,L_n$ form an almost-sure partition of $O$ if $L_1,\dots,L_n$ are pairwise disjoint and $\probm(\{\omega\in O\mid \omega \not \in L_1\cup\dots \cup L_n\})=0$
%\end{definition}

\begin{definition}[Lexicographic Ranking Supermartingale]
\label{def:lexrsm}
Let $(\Omega,\genfilt,\probm)$ be a probability space, 
$\{\genfilt_i\}_{i=0}^{\infty}$ a filtration of $\genfilt$, $\stime$ a stopping 
time w.r.t. that filtration, and 
$\eps\geq 0$. 
An $n$-dimensional real-valued stochastic process 
$\{\vec{X}_{i}\}_{i=0}^{\infty}$ is a 
\emph{lexicographic $\eps$-ranking supermartingale for $\stime$ 
($\eps$-LexRSM)} if the 
following 
conditions hold:
\begin{compactenum}
\item For each $1\leq j \leq n$ the 1-dimensional stochastic process 
$\{\vecseq{X}{i}{j}\}_{i=0}^{\infty}$ is adapted to 
$\{\genfilt_i\}_{i=0}^{\infty}$.
%\item For each $i\in \Nset_0$ and each $1\leq j \leq n$ it holds 
%$\E[|\vecseq{X}{i}{j}|]<\infty$.
\item For each $\omega \in \Omega$, $i\in \Nset_0$ and $1\leq j \leq n$ it holds 
$\vec{X}_i (\omega)[j]\geq {0}$. % and $\vec{X}_i(\omega)[j]\neq \infty$.
\item For each $i\in \Nset_0$ there exists a partition of the set $\{\stime>i\}$ into $n+1$ subsets $L^i_1,\dots,L^i_n,L^i_{n+1}$, all of them $\genfilt_i$-measurable, such that for each $1\leq j \leq n$:
\begin{compactitem}
	\item $\E[\vecseq{X}{i+1}{j}\mid 
	\genfilt_i]\leq\vecseq{X}{i}{j} - 
	\eps$ on $L_j^i$; 
	\item 
	 for all $1 \leq j' < j$ we have $\E[\vecseq{X}{i+1}{j'}\mid 
	 \genfilt_i]=\vecseq{X}{i}{j'}$ on $L_{j}^i$; and
	\item $\E[\vecseq{X}{i+1}{j}\mid 
	\genfilt_i]\leq\vecseq{X}{i}{j}$ on $L_{n+1}^i$. 
\end{compactitem}
\end{compactenum}
The $n$-dimensional LexRSM is \emph{strict} if $L^i_{n+1}=\emptyset$ for each $i$.
\end{definition}

An \emph{instance} of an $n$-dimensional LexRSM $\{\vec{X} \}_{i=0}^{\infty}$ is a tuple $(\{\vec{X}_{i=0}^{\infty},\{L_1^i,\dots,L_{n+1}^i\}_{i=0}^{\infty})$ where the second component is a sequence of partitions of $\Omega$ satisfying the condition in Definition~\ref{def:lexrsm}.
Intuitively, the sets $ L_j^{i}$ for $1\leq j \leq n$ represent the lexicographic ranking condition, i.e. for strict LexRSMs, we are in each step able to partition $\Omega$ into subsets such that on $j$-th subset the $j$-th component of $\vec{X}$ is expected to decrease while the previous components are not expected to increase. On additional sets $L_{n+1}^i$, none of the components is expected to increase, but decrease is not required: this will become handy later when we deal with compositional LexRSM-based proofs (Section~\ref{sec:compositional}) -- we will not require decrease in every step as long as decrease happens at least once on each cycle in the pCFG. We say that $\omega\in \Omega$ has level $j$ in step $i$ of instance $(\{\vec{X}_{i=0}^{\infty},\{L_1^i,\dots,L_{n+1}^i\}_{i=0}^{\infty})$ if $\omega\in L^i_j$. We also say that $\omega$ has level $0$ in step $i$ if $\stime(\omega)\leq i$.

While lexicographic ranking supermartingales provide a multidimensional extension of ranking supermartingales, 
there is an important difference even in the single dimension, which we explain below.
The strict 1-dimensional lexicographic $\eps$-ranking supermartingale is, to a large extent, equivalent to the notion of a ranking supermartingale as studied in~\cite{HolgerPOPL,CFNH16:prob-termination}. There is one significant difference: in these works there is an additional \emph{integrability} condition imposed on the one-dimensional process $\{X_i\}_{i=0}^{\infty}$, which requires that for each $i\geq 0$ it holds $\E[|X_i|]<\infty$ (or equivalently $\E[X_i]<\infty$, as the process is required to be non-negative). We do not impose this condition, which simplifies possible application of LexRSMs to programs with non-linear arithmetic, where, as already shown in~\cite{HolgerPOPL}, integrability of program variables is not guaranteed.
%We do not impose this condition, and indeed, it is possible to define a 3-dimensional LexRSM $\{\vec{X}_{i}\}_{i=0}^{\infty}$ such that $\E[X[3]_i]=\infty$ for some $i$ (example in \AppendixMaterial). 
The reason why integrability condition can be dropped is that it is only needed in the previous works to ensure that the conditional expectations exist and are well-defined. However, the existence of conditional expectations is also guaranteed for random variables that are real-valued non-negative, see Proposition~\ref{prop:conditional-exp-existence}. 
%This is exactly the case in both the original 1-dimensional ranking supermartingales and our generalization to LexRSMs. 

The following theorem states our main mathematical result on LexRSMs.
%Below we show, that also in the LexRSM case we do not need integrability to prove a.s. termination, and hence we drop the condition altogether. This is particularly relevant when applying LexRSMs to programs with non-linear arithmetic, where integrability of program variables is not guaranteed.

\begin{theorem}
\label{thm:lexrsm-main}
\label{THM:LEXRSM-MAIN}
Let $(\Omega,\genfilt,\probm)$ be a probability space, 
$\{\genfilt_i\}_{i=0}^{\infty}$ a filtration of $\genfilt$, $\stime$ a 
stopping 
time w.r.t. that filtration, and $\eps>0$. Assume there exists an $n$-dimensional $\eps$-LexRSM for $\stime$ and its instance $(\{\vec{X}_{i=0}^{\infty},\{L_1^i,\dots,L_{n+1}^i\}_{i=0}^{\infty})$ such that $\probm(\{\omega\in \Omega \mid \text{level of $\omega$ is $<n+1$ in infinitely many steps}\})=1$. Then $\probm(\stime<\infty)=1$. In particular, if there exists a strict $\eps$-LexRSM for $\stime$, then $\probm(\stime<\infty)=1$.
\end{theorem}
\begin{proof}
The proof proceeds by contradiction, i.e. we assume that an $\eps$-LexRSM for 
$\stime$ satisfying the above conditions exists and $\probm(\stime=\infty)>0$. For succinctness we denote the 
set $\{\omega\mid \stime(\omega)=\infty\}$ by $\genRunSet_{\infty}$.

For $\omega\in \Omega$ we denote the level of $\omega$ at step $i$ by $\levelrank{\omega}{i}$.
%equal to either $0$, if $\stime(\omega)\leq i$, 
%or otherwise to the $1\leq j \leq n$ such that $\omega\in L^i_j$. 
The value $\levelrank{\omega}{i}$ is well-defined for all 
$\omega$ and moreover, the random variable $\levelrank{}{i}$ is $\genfilt_i$-measurable. We denote by $\minlev(\omega)$ the smallest $0 \leq j \leq n$ such 
that $j$ is a level of $\omega$ at infinitely many steps. Note that $\omega \in 
\genRunSet_{\infty}$ if and only if $\minlev(\omega)\neq 0$, so $\probm(\genRunSet_{\infty})=\probm(\{\minlev \neq 0 \})$. We denote by $M_i$ 
the set of all $\omega$'s with $\minlev(\omega)=i$.

Throughout the proof we use several times the following fundamental fact: if 
$\probm(A)>0$ for some set $A$ and $A=A_1\cup A_2 \cup A_3\cdots$ for some 
sequence of sets $A_1,A_2,A_3,\dots$, then there exists $i$ such that 
$\probm(A_i)>0$.

Now $\genRunSet_{\infty}=M_1\cup\dots\cup M_n\cup M_{n+1}$ and $\probm(M_{n+1})=0$ (as the measure of $\omega$'s that have level $<n+1$ in only finitely many steps is zero, per Theorem's assumption), there must be $1\leq \fixn{j} \leq n$ 
s.t. $\probm(M_{\fixn{j}})>0$, i.e. with positive probability the smallest level appearing infinitely often is $\fixn{j}$. For each $\omega\in M_{\fixn{j}}$ there is the smallest number
${i}_{\omega,\fixn{j}}\in\Nset_0$ such that for all $i\geq {i}_{\omega,\fixn{j}}$ it holds 
$\levelrank{\omega}{i}\geq \fixn{j}$, i.e. after step ${i}_{\omega,\fixn{j}}$ the level of $\omega$ in all steps up to infinity is at least $\fixn{j}$. Denote by $S_{\fixn{j},{i}}$ the set of all $\omega$'s in $M_{\fixn{j}}$ s.t. 
$i_{\omega,\fixn{j}}={i}$. 
Since $M_{\fixn{j}} = M_{\fixn{j},1} \cup M_{\fixn{j},2} \cup M_{\fixn{j},3} \cup \cdots$, there is $\fixn{i}\in 
\Nset_0$ 
s.t. 
$\probm(M_{\fixn{j},\fixn{i}})>0$. That is, there is a point in time such that with positive probability, after this point the level of $\omega$ is at least $\fixn{j}$, and it is equal to $\fixn{j}$ infinitely many times. Continuing on the same note, for each $B\in \Nset$ we 
denote by $M_{\fixn{j},\fixn{i}}^{B}$ the set off all $\omega$'s in $M_{\fixn{j},\fixn{i}}$ s.t. 
$\vecseq{X}{\fixn{i}}{\fixn{j}}(\omega)\leq B$. Since $M_{\fixn{j},\fixn{i}}= M_{\fixn{j},\fixn{i}}^{1} \cup M_{\fixn{j},\fixn{i}}^{2} 
\cup M_{\fixn{j},\fixn{i}}^{3} \cup \cdots  $, there is $\fixn{B}\in \Nset$ s.t. 
$\probm(M_{\fixn{j},\fixn{i}}^{\fixn{B}})>0$. 
%Denote $\fixn{p}=\probm(M_{\fixn{j},\fixn{i}}^{\fixn{B}})$.

So there is a set of positive probability $M=M_{\fixn{j},\fixn{i}}^{\fixn{B}}$  such that for all $\omega$'s in the set: after step $\fixn{i}$ the level of $\omega$ is at least $\fixn{j}$ (which, intuitively, means that $\vec{X}[\fixn{j}]$ does not have a tendency to increase after this time step on $\omega$'s in $M$), the level of $\omega$ is infinitely often equal to $\fixn{j}$ (intuitively, $\vec{X}[\fixn{j}]$ has infinitely often the tendency to decrease by $\geq \eps$ for $\omega$'s in $M$), and at time $\fixn{i}$ the value of $\vec{X}[\fixn{j}]$ is bounded (by $B$) on $M$. This should, again intuitively, lead to a conclusion, that when ``restricted to $M$'', $\vec{X}[\fixn{j}]$ has tendency to decrease unboundedly over time, a contradiction with non-negativeness of $\vec{X}[\fixn{j}]$. However, proving this intuitive result is much more intricate: most importantly, it is not clear what ``restricted to $M$'' stands for. The stochastic process $\{\vec{X}\}_{i=0}^{\infty}$ as well as the LexRSM conditions are tied to the filtration $\{\genfilt_i\}_{i=0}^{\infty}$, but the set $M$ is not necessarily $\genfilt_i$ measurable for any concrete $i$, since whether $\omega$ belongs to $M$ depends on values of $\levelrank{\omega}{i}$ for \emph{infinitely many $i$}. Hence, we use a work-around.

Let $D$ be the set of all $\omega\in \Omega$ such that 
$\vecseq{X}{\fixn{i}}{\fixn{j}}(\omega)\leq \fixn{B}$. Note that $M \subseteq D$ and $D\in 
\genfilt_{\fixn{i}}$ (and thus also $D\in \genfilt_{i'}$ for all $i'\geq \fixn{i}$). 
Define a stopping time $F$ w.r.t. filtration $\{\genfilt_i\}_{i=0}^{\infty}$ as follows: for all $\omega \in \Omega$ 
we put 
$F(\omega)=\inf\{k\in \Nset_0 \mid k\geq \fixn{i} \text{ and }
\levelrank{\omega}{k} < \fixn{j} \}$.

Define a 
(one-dimensional) stochastic process $\{Y_k\}_{k=0}^{\infty}$ as 
follows:
$$
Y_k(\omega) = \begin{dcases}
0 & \text{if $\omega \not\in D$}\\
\fixn{B} & \text{if $\omega \in D$ and $k<\fixn{i}$ }\\
\vecseq{X}{k}{\fixn{j}}(\omega) & \text{if $\omega \in D$, $k\geq \fixn{i}$ and 
$F(\omega)>k$ 
}\\
\vecseq{X}{F(\omega)}{\fixn{j}}(\omega) & \text{if $\omega\in D$, $k\geq 
\fixn{i}$ and 
$F(\omega)\leq k$ }.
\end{dcases}
$$

Intuitively, the process $\{Y_k\}_{k=0}^{\infty}$ is an over-approximation of what we would like to call ``$\vec{X}[\fixn{j}]$ restricted to $M$.''
We prove several properties of the process. 
First, clearly for all $k\geq 0$, $Y_k(\omega)\geq 0$. Second, for each $k\geq \fixn{i}$, the variable $Y_k$ is $\genfilt_k$-measurable, as $D\in \genfilt_{\fixn{i}}$, $\{\vecseq{X}{i}{\fixn{j}}(\omega)\}_{i=0}^{\infty}$ is adapted to the filtration $\{\genfilt_i\}_{i=0}^{\infty}$ and $F$ is a stopping time w.r.t. this filtration. Finally, for any $k\in \Nset_0$ denote by 
$\noofdec_k$ the random variable such that $\noofdec_k(\omega) = |\{i'\in 
\Nset\mid \fixn{i} \leq i' < k \text{ and } \levelrank{\omega}{i'}=\fixn{j}\} |$, i.e. $\noofdec_k(\omega)$ counts the number of steps between $\fixn{i}$ and $k$ in which level is $\fixn{j}$.
We prove
that for each $k\geq \fixn{i}$ 
it holds 
\begin{equation}
\label{eq:lexrsm-soundness-main}
\E[Y_k]\leq \fixn{B}\cdot \probm(D) - \eps\cdot\sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D 
\cap \{F\geq k\} \cap \{\noofdec_k 
= 
\ell\}).
\end{equation}

The proof of~\eqref{eq:lexrsm-soundness-main} goes by induction on $k$. The computations being somewhat technical, we defer them to \AppendixMaterial. 

Now according to~\eqref{eq:lexrsm-soundness-main} it holds  $\E[Y_k]\leq \fixn{B}\cdot \probm(D) - \eps\cdot\sum_{\ell=0}^{k-\fixn{i}} \ell\cdot\probm(D 
\cap \{F\geq k\} \cap \{\noofdec_k 
= 
\ell\})$ for all $k\geq \fixn{i}$. Let $m=3\fixn{B}\cdot \probm(D)/(\eps\cdot \probm(M))$. For each $\omega\in M$ we see level $\fixn{j}$ infinitely often, so there exists step $k(\omega)\geq \fixn{i}$ such that $\noofdec_{k(\omega)}\geq m$, i.e. $\omega$ has level $\fixn{j} $ at least in $m$ steps between steps $\fixn{i}$ and $k(\omega)$. Clearly, $M = \bigcup_{\ell=\fixn{i}}^{\infty} (M \cap \{k(\omega) \leq \ell\})$ and hence $\probm(M)=\lim_{\ell\rightarrow \infty}\probm(M \cap \{k_{m} \leq \ell\})$. Thus, there exists $\ell_0 \geq m+\fixn{i}$ such that $\probm(M \cap \{\noofdec_{\ell_0}\geq m\}) \geq \probm(M)/2$. Clearly $M \cap \{\noofdec_{\ell_0}\geq m\} \subseteq D\cap \{F\geq \ell_0 \} \cap \{\noofdec_{\ell_0}\geq m \}$. From \eqref{eq:lexrsm-soundness-main} it follows that
\begin{align*}
\E[Y_{\ell_0}]&\leq \fixn{B}\cdot \probm(D) - \eps\cdot\sum_{\ell=0}^{\ell_0-\fixn{i}} \ell\cdot\probm(D 
\cap \{F\geq \ell_0\} \cap \{\noofdec_{\ell_0} 
=
\ell\}) \\
&= \fixn{B}\cdot \probm(D) - \eps\cdot \sum_{\ell=1}^{\ell_0-\fixn{i}} \probm(D 
\cap \{F\geq \ell_0\} \cap \{\noofdec_{\ell_0} 
\geq 
\ell\})\\
& \leq \fixn{B}\cdot \probm(D) - \eps\cdot \sum_{\ell=1}^{m} \probm(D 
\cap \{F\geq \ell_0\} \cap \{\noofdec_{\ell_0} 
\geq 
\ell\}) \\&\leq \fixn{B}\cdot \probm(D) - \eps\cdot \sum_{\ell=1}^{m} \probm(D 
\cap \{F\geq \ell_0\} \cap \{\noofdec_{\ell_0} 
\geq 
m\})\\
&\leq \fixn{B}\cdot \probm(D) - \eps\cdot m \cdot \probm(M)/2 < 0, 
\end{align*}where the second line follows by standard re-arranging of terms, the third line follows from the fact that $m\leq \ell_0 + \fixn{i}$, the fourth line follows from $\{\noofdec_{\ell_0} 
\geq 
m \} \subseteq\{ \noofdec_{\ell_0} 
\geq 
\ell\}$ for each $\ell \leq m$, the first inequality on the last line follows 
by  using $\probm(M)/2\leq \probm(M \cap \{\noofdec_{\ell_0}\geq m\}) \leq 
\probm(D\cap \{F\geq \ell_0 \} \cap \{\noofdec_{\ell_0}\geq m \})$, and
the last inequality follows by expanding the definition of $m$. But for each $k$ the random variable $Y_k$ is non-negative, so it must also have a non-negative expectation, a contradiction. Finally, note that for strict $n$-dimensional LexRSMs the condition of level $<n+1$ appearing infinitely many times is trivially satisfied.
\end{proof}

